I'm going to explain how to fine-tune the latest open- source models all the way from Gemma 3, Quen 3, Lama 4, FI4, and Mistral Small. I'll explain the pros and cons of using Unsloth versus using transformers. I'll explain also how to do fast evaluations using VLM separate to the finetuning. And then I'll go through in detail some of the techniques around how to set the hyperparameters to get the best results. For a video overview, I'll briefly describe why you might want to fine-tune. Hopefully, you've got a sense already uh if you're watching this video, but I'll recap on that. I'll very briefly describe how to prepare data. You probably should be spending 90% of your time on data preparation, and I have a lot of a lot of videos covering that. I will link them here, but it's not going to be the focus of this video. Then I'll talk about Unsloth, which is a wrapper on transformers that brings significant improvements, but I'll talk about the pros and cons of using unsloth versus transformers for fine-tuning. Then I'll talk a little about running fast evaluations. You should be evaluating your performance before you fine-tune and afterwards and during as well. And I'll show you how you can speed that up a lot by using VLM in the same notebook as you're doing the fine-tuning.
Then I'll talk about which model to fine-tune out of the available open source ones considering factors like the license licenses and performance. I'll give a few general fine-tuning chip tips before then going to a live demo of fine-tuning using onslaught and transformer notebooks that I've put together in the advanced fine-tuning repo. So why should you fine-tune? Generally this is ideally a last resort. You have tried out doing prompt engineering. You have included retrieval in your techniques and you still need to improve performance. Often that means one of a few things. You need to improve your answer structure and format. For example, you want the model to take a certain approach by maybe recapping on some of the information, processing it and uh maybe reasoning over it and then giving a final structured answer. Sometimes that means you have uh tool calling. Uh so you want to have a structured response that is going to call an assistive tool. Uh so these are classic cases where fine-tuning can make sense because it gets the model to very consistently respond in a certain format. Now there are two others you might consider as well. One is if you want to improve accuracy beyond just using a retrieval method. Back in one of my earlier videos I show how you can combine retrieval with fine-tuning and get the best possible approach. So you can improve performance beyond just a retrieval approach only. Uh and last of all this is more modern uh late 2024.
You can try to fine-tune for a specific uh for specific reasoning within a domain that's like uh GRPO group relative policy optimization and other related techniques. I have a series of videos from earlier in 2025 uh that cover that and I'll let you try them out there. For today though, we're going to focus uh not so much on reasoning, although I think I will make another reasoning video soon. We're going to focus on general fine-tuning to enhance uh knowledge. But uh a lot of the same principles apply if you wanted to fine-tune on structure, although I'll point you for uh more details on fine-tuning for uh JSON responses or function calling to this video. uh right here I'm not going to talk too much about data preparation uh but I will point you to well I'll give you a few tips and point you to the key videos basically there are two types of training and two types of data sets correspondingly at a very simple level there's what's called continued pre-training with raw data this is where you take say magazine content newsletter articles or books and you feed that in without much uh pre-processing I mean you'll clean it up but you're not going to change it into Q&A pairs um this continued pre-training here.
Typically, this is difficult to do on top of an existing model because it can tend to undo the instruction type training that a model has. So often, unless you have a very large amount of data and you're willing to do continued pre-training followed by post- trainining, then it's probably not recommended to try and do the continued pre-training. Instead, what generally is recommended, particularly with smaller amounts of data, like say up to maybe 100,000 uh words or even a million, you do post training on question and answer type data sets. And these are often data sets you synthetically create using documents and using LLMs to create questions and answers from those documents. Now, to prepare a synthetic data set, it's recommended to use a large language model. um you want to generate not just questions and answers but ideally questions evaluation criteria. So what the criteria are for correct answers and then highquality answers. I have a video that came out recently around how you can prepare data this way. If you further want the answers to involve reasoning or to involve chain of thought, you probably need to augment them further. And I do have a video showing how with these augmented answers, you can get up to very high levels of accuracy with fine-tuned models. And that's true whether you're fine-tuning open source or using APIs for fine-tuning models like OpenAI's API. Now, one caveat, um, I don't say this with 100% confidence. Uh, maybe maybe 75% confidence. My sense is that as models get stronger, it's harder to fine-tune. And that's because the model starts off from a very good point. And in fine-tuning, you are risking damaging the model in some ways. I'll give you a simple example. If you take a reasoning model, it might perform quite well. And if you train it without reasoning data, you could just drag the performance below what the reasoning performance was even though you're adding the right content to it. So I think in some ways fine-tuning is getting a bit trickier.
And if you are going to try and fine-tune for reasoning um and I mean reasoning not in a say technical application where you can use um GRPO or even SFT like I described in one of my previous videos. Uh let's see this one here. But if you're trying to use reasoning in kind of a verbal domain that's going to be tricky. It's something I want to cover maybe in a later video. But just kind of watch out because if you uh don't have proper reasoning data sets developed, you do risk maybe regressing performance. Okay, so I'm going to talk about the libraries. There are quite a few libraries out there. Some of the most common are um at least used at the smaller scale are unsloth and transformers. There are also libraries like Axodal um torch tune. Um those are two other examples.
Maybe I'll go through those at some point. But the one that I've been using most often on this channel is either Transformers or Unsloth. Unsloth is effectively a wrapper. I mean, I don't mean that in a negative way. Um, it's brought a lot of improvements to the transformers library in terms of speed and just ease of use. So, I want to just recap those here. I'm going to show you notebooks that do unsloth only and then transformers only, but it's worth appreciating the differences. So, Unsloth generally is two times faster than transformers. Uh and that's because of a variety of tricks. It's not just one thing. It's like the accumulation of about five or six tricks that result in faster fine-tuning. Also, Unslot provides a unified function for loading uh multimodal models. So, if you have a model like JAMA 3, which is multimodal in sizes larger than 4B, you need to change the function that you use to import that model if you're going to use transformers. Whereas with Unslo, it's just unified how you load that model. So that makes it a lot easier to have one script that supports a lot of different models. Onslaught for now is single GPU only. So if you have a very large model, you might need to use um transformers instead, which does support multiGPU. By default, transformers will use um model parallel. So it will chunk up the model by layers, which means your GPUs are not all active when you're fine-tuning because you will use one GPU to process these layers, then the next GPU, then the next. Um that might sound inefficient and it is but it's quite simple and robust. So I wouldn't rule that out. If you need to tune a larger model just uh use that simple approach.
Now you can take improved approaches called fully sharded um using a fully sharded data parallel approach. That's where you split the model uh across GPUs but you split the matrices essentially so that all GPUs are being used more or less at once. And you can find a video on fully sharded data parallel on this channel if you just look up FSDP. Something else about uh unsloth because it's a wrapper sometimes there are issues where transformers will move ahead and maybe Unslaught does not quite support that. Also, if you're trying to use AI to help you code, there will tend to be more documentation on the advanced features in transformers. And because everything is being wrapped by Unsloth, it's sometimes harder to access the features by working through Unsllo. So, basically what I'm saying is if you're trying to use some more obscure functionality and transformers, it may be easier to use transformers than to try and use unsloth and have to figure out how Unsloth has wrapped that. For now also, uh, JAMAMA 3 is still broken in the sense that the configuration file won't allow you to run inference on VLM.
So, if you do fine-tune a Jamma 3 model on Unslaught, um, I'll point you to an issue where there's uh, potentially a fix. Uh, I expect that it will be fixed at some point, but something to keep uh, a note on. And also, you should note that GPUs are getting very big. like a B200 which you can rent on RunPod now I think for $8 an hour it's uh 192 GB in VRAM so if you have a model you can probably fit a model that's up to 150 um billion parameters in 8 bits on allows you to fine-tune in 8 bits now not just in four bits I don't recommend 4-bit I know it's very popular doing Qura but I have found particularly because of merging back adapters you can see small differences in performance that are hard to predict. So I generally recommend fine-tuning in 16 bits. Uh 8 bit is probably uh is probably quite good as well. So yeah, if you have a model that's 16 bits, you can train up to probably 80 or 70 billion parameters in on a B200 with Unsllo or in 8 bits, probably even something larger. So this is not even that much of a limitation.
You're not going to be able to uh fine-tune Deepseek using Unslaught. Uh now DeepSeek anyway is probably hard to fine-tune even with transformers. But uh yeah, this is a limitation. If you wanted to fine-tune, let's say Llama 4 and the Maverick version, you're not going to fit that on a single GPU. But you could fit the Scout version in a single GPU in 8 bits because it's 100 uh billion parameters roughly and it's about one bite per parameter. So in eight bits, so that would be fitting onto that GPU. Just a note on running evaluations. So you want to run evaluations before you fine-tune and after and ideally during to see if your fine-tuning is working. You can just run inference using transformers or unsloth. But neither of these libraries are designed for inference. So they don't do what's called continuous batching. You can send in a batch of tokens. Uh so you have to size the batch so you don't run out of memory. um you can slightly automate that actually true transformers by having um a test for the right batch size and it will reduce it if it's too large but the inference is just not optimal in terms of the back end and it will be significantly slower than using something like VLM or SG lang so I recommend and that's why the scripts I'll show you today they have two parts they've got an inference part or an evaluation part that's run with VLM and then they've got a fine-tuning part that's either transformers or onslaught and it's much faster to use VLLM M the drawback is you have to reload the model in VLM after you fine-tuned in unslothe transformers. So there's kind of this trade-off. If you have a very big model, it can take a bit of time to load. But again, if you're only on one GPU, this is probably not going to be a big constraint and reloading the model should be fairly fast given it's already going to be on your disk from the fine-tuning.
Now for some questions on which model to fine-tune and I've listed them in a tentative order of preference. Mistral I think is Mistral small. It's um less than 30 billion parameters. It's an Apache 2 license and it tends to be strong in evaluations just I've heard from customers as well when I see results across the different models they've tried. So it would be one of my top recommendations. Gemma 3 is a very strong model as well, but the license is custom. Uh, so if you're at a bigger company where there's sign off on the general open source licenses, but there needs to be review of custom licenses, this is maybe adding a little bit more friction. Um, and I've added some notes here. You can just click on these links to get more info on the licenses. 54 from Microsoft is a permissive license. It also allows for reasoning. These top two models don't.
So if you want to fine-tune for reasoning, maybe 54 is a good option.
Lam 4 is a custom license and it's also very large. The scout model is 100 billion parameters and I think it's unnecessarily big for the quality it provides. The quality is probably not much better than Gemma uh 27B Gemma 3 or potentially even Mistral. So I recommend probably just using Mistral or Gemma 3 over uh Lama 4. Quen 3 is a very strong model. I think it is probably stronger than all of these models here and it's Apache 2. But you do have the issues that come with using quen or deepseek models that there is strong censorship of the models and also there is a backdoor risk uh with with any language model here. Um but you have to weigh that in the context of where and how it was developed. These models are increasingly being used to control agents and that provides an extra angle by which you can have danger if the model is uh controlling your agent in a malicious way and making tool calls that uh you don't want to be made. So quen 3 very strong perhaps very good choice if you want to fine-tune for reasoning but you do have to be careful of the censorship and the backdoor issues. Now just a few general fine-tuning tips before I move to a demo.
spend 80 or 90% plus of your time on data preparation. That probably means uh watching some of the other videos.
Second of all, define two evaluation data sets. One is a representative data set that's not in your training set. I explained in my data prep video how you can rephrase certain questions in order to make sure they're not verbatim in your training and your eval set. But I do I do recommend including a verbatim copy of some of your training data set because by including a verbatim copy and also a version a data set that's not in your training set, you can start to measure the difference in performance between these two and assess whether you are overfitting. Measuring overfitting is another reason to use the eval set during training. So you should be calculating training and eval losses.
Make sure to evaluate before and after fine-tuning. Um, and then one kind of random tip here is do inspect the chat template being used when you're fine-tuning. Some chat templates have got the date included with them. And if they have the date, you probably don't want to be fine-tuning on today's date for all of your examples. Uh, so you may want to remove that uh when you're doing the finetuning. Uh, so other things that can appear unexpectedly may also adversely affect the quality. Okay, so that's it for the theory portion. I'm going to move now and show you how to fine-tune. And if you want to find the scripts I'm going to show you, they're available at trellis.com and then advanced fine-tuning. And I've actually refreshed the repository. Let me show you here.
I've just cloned it over using windsurf.
Um historically the fine-tuning repo was organized according to branches. So different branches would have different content and those branches are still there. For example, if you want uh synthetic scripts, scripts for making a synthetic data, synthetic branch, distillation, uh low vramm full fine-tuning, retrieval, rag, um using Wikipedia data for fine-tuning.
There are a large variety of scripts and they are still there in different branches. But going forward I'm going to leave all of the scripts within uh the main branch here and I've started to create uh a clean folder for data prep.
That was the most recent video. And now I've got a clean folder here for fine-tuning. Um and this is in VLM onsloth but I'm going to merge it into main. So you'll be able to find it there after the video. Now there are three fine-tuning scripts that I have prepared.
One is VLM Unsloth. It uses VLM for evaluation and then Unsllo for fine-tuning. This one here is VLM transformers. So it uses unslot for eval sorry VLM for eval and transformers for fine-tuning. And then this one here is pure transformers. So it will just use transformers both for evaluation and for fine-tuning. And it will automatically set the right batch size for evaluation.
But the evaluation is quite a bit slower than if you're using uh VLM. So I'm going to show you a fine tune using uh the unscloth script here.
The transformer script more or less uh follows it. And when I go through this I'll highlight a few points where there's a difference between using unsloth and uh the fine-tuning. Now to get going on a GPU, I'm going to use runpod and this oneclick template affiliate link I have here.
If you want to exactly replicate the environment I'm using, you can use this and pick a GPU. Now, here um I said 192.
That was wrong. Sorry, I meant I guess um what I should have said was 180. So, if you run a B200 for seven, uh looks like yeah, $8 an hour. Maybe I said seven. You can fit 180 GB of VRAM. We're just going to run with a H100, which is 80 GB. Uh we won't be running quite as big a model. So, I'll just say fine-tuning with Unsloth as the name.
And you can see here that we've got Auda 12.1 and PyTorch 2.2 uh template. So, I'll get this going. Okay. So, once the pod is started, we're going to connect and open up Jupiter and then I'm going to upload my notebook uh the Unsloth one. Now just when I'm uploading Unsloth here, notice that I can also uh upload a requirements file. So you can either install the latest version of the dependencies or you can install from the requirements file if you want to ensure reproducibility. Okay, so I apologize for hurting the eyes of those who are sensitive to white light. I have swapped this over to dark mode and we're going to start off with an evaluation. Throughout this video, we're going to use uh a hugging face data set. Well, it's a trellis data set uh called touch rugby and comprehensive.
It's the QA data set I generated in a recent video. And it consists of a series of questions and answers and then evaluation criteria for marking uh a given answer correct.
And these were all generated uh using I think the Gemini Pro 2.5 model based on a document with the touch rugby rules.
So the first thing I'm going to do is I'm going to run some uh installations here. Sometimes if you've run the fine-tuning first with Unsloth, you might need to uninstall Unsloth, but we're since we're starting fresh here, um we don't need to uninstall Unsloth.
to just run ahead and do the installs.
The most important of which is VLM here.
Now, I'm just going to train one model.
I'm actually going to train I think the FI mini instruct model 54. It's a new model I haven't trained. So, I'm kind of curious what the results will be and see if we hit any issues. But I will give some commentary as we go about uh about some of the other models. So, uh, my first comment here is if you're using the Quen model, if you want to fine-tune and disable reasoning using VLM, you do need the latest install from source of VLM. It does take quite a bit of time to install. So, just a heads up. Um, if you're going to use Quen 3 and you want to disable read reasoning, you need to install VLM for s from source for now.
All right. So, while that's installing, uh, we'll move on here. do restart the kernel after the installs are done. Uh this is where I was saving my requirements to a txt file. You could if you wish rather than running the installs here above you could have done uvp pip install- requirements uh vlm sloth like this vlm on slot and then dash system. Now the dash system means that we're installing onto um the system. We're not in a virtual environment. And this makes sense because when we started the Docker image on this GPU, we already had CUDA and PyTorch installed and we want to make use of those. We don't want everything to be packaged in a VM. Okay, so this is still installing.
Installs are done. So I'm going to restart the kernel. And now I'm going to log into Hugging Face. So I'm going to I won't add it as a Git credential. I will get a token. And I've got a token and we're logged in now. I can save that at least close it. And the model that we're going to evaluate. So we'll evaluate the base model. Then we'll do fine tuning and then we'll evaluate again. So the model I want to test is going to be a base model. And I'm going to set model slug equal to uh the fi model. And I've got the fi model up over here. And yeah, the dark mode is pretty poor on Safari.
So I'll paste it in and the data set is the touch rugby data set. The training split is called train. The eval split is called evval. There is a mirror evval split. This is literally a subset of the training set. So we can test overfitting. Uh I talked about that in the data set prep video. And within this data set, there's a column for question.
There's a column for evaluation criteria which we'll use for grading. And there's a column for answer which we'll use for fine-tuning. So the answers for fine-tuning the evaluation criteria is for grading and for setting up the judge we're going to use Gemini flash Gemini 2.0 we're going to provide a long context length um it can be shorter but if you're using reasoning you need more length and we'll use a low temperature uh for the grading here. This allows for fast weight downloads and this makes sure that we download the weights into the disk. So we want when we're on runpod we want the weights to go on the volume. We don't want it to go on the container. The volume here is about 500 gigabytes. The container is only I think maybe 10. So I'll run this. That just sets those variables.
And now we're going to set up the judge.
Now it's asking me for an API key for Gemini, which makes sense because we're using the Flash model. So I'm just going to go over uh to AI Studio. I'll do this offscreen and I'm going to create an API key. So I'll paste in that API key and that's set now. And just to briefly show you the judge, um this is not yet the judge. It's just setting the API key.
It's actually saving it to a local environment variables file so that if I rerun this cell, it won't force me to put in the API key again, which is nice.
Um here we're setting up an OpenAI client. So, we're hitting Gemini using an OpenAI client. And here we've just set up uh this function called chat that allows us to send messages in um with a single message into Gemini. Okay. Next, we're going to prepare the data set. And when we prepare it, we can turn a test mode on.
You'll notice that back here earlier when I defined the data set, uh there's a flag to set test equals to true. If you just want to look at a small sample of the data set, you can set test equals to true. And it will also print a lot more debug logs down below in the script. So I've got that set to false and we're going to load the data set with the data set name. Check if there's train and eval splits. And yeah, if we're just inspecting, we're going to slice the data so that we're only going to take a few rows. So here we're potentially sampling if test is true otherwise we're loading the full data set and we're just printing out to make sure everything is in order here. So when I print the data in eval set the train and eval set there should be around yeah 244 training rows and then 32 in the eval split. Okay. Now we're going to load the model to run inference. This is how you load with VLM. You pass in the model slug, the GPU maximum memory utilization, the data type, and the max sequence length, which we set up earlier, and our sampling parameters. So, here when we're generating, we're going to use a temperature um that's defined by temperature here, which I think I've set to 7. Uh top K of 40 and top P of um 0.95. These are important because it makes sure that very low probability tokens are not accepted. You don't want tokens that are very low probability because they can throw your prompt or they can throw your completion off in an unexpected direction. So at this point um the model is going to be loaded.
It'll be downloaded from hugging face first and then the shards will be loaded onto the GPU. So this is typically where you hit problems if you're going to hit problems when you're trying to um load a given model. So here you can see the smaller files have been loaded quickly.
And now we're going to load uh the safe tensors. So you can see there's about 7 GB. And notice here that we are using flash infer. Flash infer is relatively recent library. It's faster than uh flash attention. And if you have it installed, it will be used by VLM. If you don't install it, I think it won't be used by default.
So this model um fi 4 mini uh relatively small about 3 to four billion parameters in size uh pretty fast to load the weights. It's then going to create cuda graph. So it's going to calculate forward some paths for optimally doing the computations. This takes a bit of time but then it makes the inference faster because you've premputed uh what's called the graph. Okay. So when the model's loaded we can run evaluation. To run evaluation we need to get answers which is straightforward. who just pass the question to the VLM model. But then we need to evaluate that. And to evaluate it, we're going to need a prompt. Here we have an expert evaluator with uh tasked with determining if the answer satisfied satisfies the specified, excuse me, evaluation criteria. We're telling Gemma Gemini that it will receive a question, the evaluation uh criteria, and then the model's answer that needs to be evaluated. And it's just going to score one or zero. So it's either right or wrong. And the prompt template is to pass in the question, the eval criteria and the model answer. Okay, so this is pretty much it. We are going to define a helper. This is the evaluation result.
We want the evaluator to first give a reason and then indicate whether the model is correct or not. We'll then extract using a regular expression the score whether it's one or zero to determine whether it's marked correct or not. And we also have this regular expression to extract any thinking. So if you're going to evaluate the answer, the shorter the answer, the easier to evaluate. And so if you include all of the thinking along with the answer, it's going to be harder to evaluate. And for that reason, if there is any thinking, we're going to strip uh the thinking here. Then we have uh a function to evaluate and you can see it uh generates an answer.
Um sorry, this evaluate answer takes in a generated answer. It takes in a ground truth an evaluation criterion in a question and it's going to strip the reasoning and then pass it uh for judging here. Then we'll call the judge LLM and we'll get the response and parse that. And when we want to evaluate a model, here's where we need to create the messages with the problem, generate an answer, and then pass that answer in for evaluation. So that's the full loop.
And just a note here that if you want to um disable thinking, you can do that for quen 3 models by passing in this parameter here. Uh but you do need to install from source uh at least for now.
So we've defined that evaluation function and we're just going to run it on uh a row. This is the fifth row uh of evaluation data. And it looks like my API key is invalid. So, I'm going to go back up and I'm going to reset that API key. And to do that, I think I just need to search for the word reset. Okay. So, by the way, you can also select OpenAI for grading as well if you wish, but I'm going to use Gemini. And um let's now put in my API key. I'm just going to get a new key here. I must have pasted in a wrong key.
Okay, let's try that. And we'll continue on down and see if that evaluates the model for us. And here VLM has failed. And that's because I need to reload the model. I shouldn't have rerun that. I should have just gone straight to the evaluation because the model was already loaded. So I hit a a CUDA out of memory. See, VLM will often use up the full memory for when it loads a model. It will pre-allocate that memory. So it's recommended uh don't rerun the model loading. Okay. So it is going to take a second now because we need to reload the model with VLM. So yeah, before I restart the kernel, I'm just going to set that um reset back to false because I should have fixed my API key. And now I can run all the cells. So yeah, the model is reloading. And I think sometimes the c the graph can be cached. So that improves the speed for loading as well. And you can see for this maximum sequence length VLM with this GPU is capable of concurrency of 56. So it will automate batching for us like that. And here we go. It's now evaluated the question. What's the regulation about touch rugby participants covering the ball with their clothing? And here is the generated answer. And here is the evaluation criteria.
and the judge has marked it correct because it's uh saying that intentionally covering the ball is a foul. So the final evaluation is one out of one correct. So this was just evaluating one question. But what we want to do is we want to evaluate a batch of questions. So we're going to use batching. We're going to have VLM answer multiple questions in parallel and then we're going to use threading to make parallel calls to Gemini's API. So that's what's happening here. We're batch evaluating the model by building a list of conversations. That's a list of all the different questions. We're passing that in uh to VLM. We're passing it into model.hat here. Again, if you want to disable thinking, that's the line to include. And then we're going to judge using a thread pool executor that's going to make multiple parallel requests. And we'll get back uh the final score here. So, you can now run a short test. You can set test to true.
It'll just run two rows of the data set and um we can see how that does and actually sorry it's running five rows so that's why it's generated five responses and then it's going to judge those. So here's a sample generation. In fact, it's just giving us the five generations first and then it's giving us the judging of those five. And you can see the results coming out here. And it looks like we've gotten uh 42 40% correct. Now, we're going to evaluate the full evaluation data set. But because we have temperature non zero, this is not deterministic. So, actually when you evaluate, you want to evaluate multiple times on that same eval data set. Or maybe if you had a very large eval data set, you wouldn't have to do this. But because my data set is 32, you'll find some variance if you just run it once. So I recommend running it at least three times. And for that, I've got a function here that allows me to run it m number of times. So this is for running uh evaluations m number of times. And I'm going to just copy this here because I've run it previously on Gemma 3. Let's just create a new cell.
and it's going to print out the data set name, the eval split name. So, we're running the comprehensive data set, the eval split, and we're running the fi model. And uh we should be ready to go.
Now, one thing I don't like here is I'm still in test mode. So, I'm getting all of the logs here. So, actually, what I need to do is set test equals false and run it again. And that'll just suppress all of the detail logging. You can see here we're going to run on all of the prompts and then it's going to evaluate those using the judge. Now actually you could increase the number of threads here. Uh Gemini Zapi is able to take much more. You could probably even increase it up to 128 if you wanted. And I think I have an issue with the kernel because I can see my GPU memory is is not working. That may be because I just stopped it.
during processing. Sometimes if you stop VLM midway, you just run into these issues. So maybe I should have let it run out, but I didn't feel like that because it was printing too much debugging. So we'll run it again here.
And just while we're waiting for that running, I'm actually running the comprehensive data set as opposed to a manual data set that I uh curated. So this should actually be moved down. And you can see evaluation is starting there by the way. And I can probably just move it in this way here just by clicking the down button. And we can now run that full evaluation. Now this is a section for a manual data set I curated. But we're running the comprehensive data set. You can see here I ran it previously on the Mistral model. And if I just paste in a copy of this, I'm adding in test equals false to make sure we don't have too much uh debug logs. And you can see now we're processing the prompts. So 32 prompts.
And I think I actually should have put that with a lowercase test. Uh either way, we can probably wait for it to complete and then rerun it. So it doesn't print out all of the logs here because it's going to print out uh 32 of the answers. And here you can see it making those parallel calls to Gemini.
And we're running uh basically 96 different prompts here because we're running this three times. So we're running the evaluation three different times. And you can see here the data set, the eval split, and then the model name that we're running. Now I'm just going to run this again so it doesn't print the verbose logs by setting uh test equals false. And in the meantime, we can take a look at some of the results. So here with Mistral small, when I ran this three times, I got an average of 13 answers correct somewhere around 40% overall. Um, I can show you also. I think I ran on here's some archived results on the Quen 1.7B. So, the Quen 1.7B I scored five.
So, about uh half the amount, five instead of 13. And I can show you also, let's see, do I have any other results down here?
Um, Jamma 3 4B and Jamma 3 4B scores nine. So yeah, the small quen model scoring about four. That's including reasoning by the way. Sorry, five. Uh Jamma 4B scoring about nine. Mistral scoring about 13. And now let's go up and see how we score with the five model. I expect I don't know maybe something like the jam 4B somewhere around nine. Okay, we got six. Then the next one we got four. So that just shows you the variance and that's why it's valuable to run multiple times. And then the last one here uh we've scored six. So on average we're scoring five. So actually this model the five small or the five mini is not much better than the Quen uh 1.7B. So what this does is it gives us a baseline.
We've got five uh.3 correct. We're now going to run fine-tuning and we'll come back and run this again and we're going to see if we get an improvement in the performance. Now improving performance is not trivial. It's not obvious that we will improve just by doing this fine-tuning. I have not done augmentation on this data set. It's a raw set of answers that were generated by Gemini Pro which may not match probably doesn't match the kind of logits or the probability pattern of the model that we're training here. So I could definitely do a better job of improving this data. So, I'm not sure we're going to improve performance here by fine-tuning, but at least I'll be able to show you how the scripts all work. So, we'll move on down here. In fact, I'm going to minimize this section on evaluation, and we'll move to the finetuning section. Now, for running fine-tuning, we're going to need um to use Unsloth, and we're going to uninstall VLM to make sure we don't have uh conflicts. if you want to speed up uh the finetuning you can install flash attention but it does give issues with quen so that's just a little warning and if you want to use it when you load the model you need to add this here attention implementation so I'm just going to run this cell I'm not going to install flash attention for now um if I'd restarted the kernel it would have gotten rid of these warnings that's okay though it's going to uh still correctly install and I'm going to now restart my kernel and we should have onslaught uh installed.
Now, just two troubleshooting things. If you find that there's a conflict with torch vision, you do not need torch vision, I think, anymore with the latest version of transformers or unslaught.
So, you could just uninstall it. Also, if you have issues with Unslaught's cut cross entropy, unslaught has um a custom cross entropy that saves saves on memory, I think maybe on compute, you can disable it if there are issues by running this line here.
Okay, so I've restarted the kernel and I should still be logged in. Um, actually it looks like maybe I'm not fully logged in. So I'm going to go across and get a token. And the reason I want to be logged in here is so that I can uh push models up to hub or access uh private models. Now the model that we want to train is going to be the fi model again.
So, I need to populate that. And yeah, we can set the max sequence length to 8,000. We're going to fine-tune in 16 bits, which I recommend, but 8 bit, I think, actually is not bad. So, if you wanted to save memory, you could do that. We're going to use this data set, and we're going to set the name of the question column, the uh criteria column, and the answer column. So, we've done that right here. And if you wanted, you could use a different data set for Evval um by setting this to uh setting this here to the load data set right here. Okay. So, we're going to set these variables. I've got this little helper function. This is just a function to clear uh CUDA. If you've loaded a model and you want to reload a model, you can clear out the model that's there already. So we just create this little helper function. And now we're going to load the model. Uh so I'll run this here and I'm going to print out the padding side.
Um typically for fine-tuning you will want to pad on the right hand side or you'll want to use whatever the model's default is, but for inference using VLM you typically want left padding. Uh so just a note there. We're going to print this out and see what happens for fi.
Now onslaught here is downloading the model which I do not want because the model should actually already be downloaded. So what I need to do is uh potentially set this cache directory and retry. And again it's downloading the model here. So I'm going to restart the kernel and let's just check that we have the name of the model correct. 54. Yeah, it could be that Unsloth is downloading it from its own repo because Unsloth has got a version of all these models. So, I'm not entirely sure, but I suspect that may be what happening. If I look at the 54 mini model here, this is the original model, but if I copy this, there's probably an unsloth version.
Yeah, Unsloth has got this version here and it may just be defaulting to this.
So that's why it's actually downloading the model even though um I've downloaded it already for VLM. That's okay though.
We can uh keep going allow it to download and then we'll see what padding side is default and we'll also print out the model. Okay, so pretty fast. Yeah, it's not using a padding token. So actually Unsloth is automatically setting it to the end of text token and it's using uh the left hand side for padding which should be fine. And you can see the model architecture here 31 layers the MLPS the attention and actually the attention is fused. So the Q K and V are fused together. Unsloth might decide to unfuse those. I'm not sure. We'll see what happens.
So this here is a function just for me to inspect the size of these different modules. This is relevant because larger matrices you should train more slowly and this affects how we uh put on the adapters and yeah we actually need to adjust this code to work for fi because the layout is not the same. So if I go to chat GPT and I create a new conversation, I can say um update this code to support the FI architecture and sorry my screen is small there. I'm just going to paste in the FI architecture now. So I paste this in and we'll print. Now why am I going to this trouble to see the dimensions?
I'm doing it because I want to know what I should set my Laura alpha to. And the Laura alpha should be the square root of the smallest matrix dimension. Um, basically Laura alpha is it's kind of setting a bar for how you think about the relative training rate of the adapters, the lower adapters versus the main matrices. We're using Laura low rank adapters. That means we're not going to fully fine-tune all the weights. We're just going to put these little adapters that clip on to the main model and we're going to fine-tune those instead. But because they're smaller, you need to train them faster. The size of the adapters is determined by the rank. The larger the rank, the slower you want to train it.
And that's actually scaled automatically when we set use RS Laura. But you do need to set up this parameter that effectively is referencing the matrix size because it's all about the relative size of the adapters compared to that uh original matrix that we're going to freeze. Okay, so it's given us this uh unwrap function and we will see if it works. So let's see here. Is it giving us both functions? Yes.
So I think I can just paste that in. And if I run it. Yeah. So now we can see here we've got um this fused layer which has got the smaller dimension of 3,000 and we've got um the MLP which has got 3,000. So actually what we want to do here is we want square root of um 3,000. And I think that's going to work out. 32 is about square root of 1,00. So it should be about 1.7 times this. Uh so something like you know 50 is going to be fine.
And now we're going to get the parameter efficient finetune model. This is where we create the adapters. So we're going to take the model set the rank of 32.
That should be fine. If you want more granularity you can increase. Um we are not going to fine-tune any vision layers. In fact I don't know if FI supports vision in any case. Um, I don't think it does. I think it might just be text. And but if you're loading something like Jama 3, you want to set this false so you're not tuning it. Then we're going to decide to train the attention or the MLP modules. Um, if you're training ane like lama, you typically do not train the MLP. You just train attention because MLP is going to be sparse because it will be a mixture of experts. Um, so yeah, that's basically your guidance. Additionally, if you want to train the embeddings, that's often relevant if you're trying to change. Uh here, for example, we've created a pad token or we've used an EOS end of sequence token. It's probably not necessary to train the embeddings, but if you redefine some new tokens or the purpose of those tokens, then you do need to train the embeddings. Laura alpha is passed here. We're going to use gradient checkpointing. That means we're not going to store everything on the forward pass. We're only going to uh recalculate when we do the backward pass and that saves memory. We're not going to do full fine-tuning although onslaught does support that and we are going to automatically scale the learning rate of the adapters based on the size of the rank. So we're applying now or we're creating these adapters and then we're going to see how many trainable parameters we have. We have uh 4625. Now these names here the modules to save should match what we have in the model up here. So it should match lm head and embed tokens. I think this is not actually setting them to trainable for fi because the embeddings are usually large and the trainable parameters would be much larger if um if we were actually training these.
So I don't think we're actually training the embeddings here. We're just training uh the lower adapters. I also want um to see what modules are set as trainable.
So let's just ask here give me a function to see what modules are set to trainable in the model because I suspect that because the QKV are fused either on sloth has to unfuse them or is not going to set them trainable. But I'm not sure about that.
Maybe it will. So let's uh add this here and run. And we do need to pass the model. Okay.
So yeah, it looks like the MLP layers are being trained. And that's pretty much it. Oh, the O projection. Yeah, that makes sense. But the QKV is not because it's fused. So yeah, basically um Unsloth is not unfusing here. So it means that we're only training one of the modules in the attention. Uh we're not actually training the full thing. Okay. So, we've got the model, we've got the adapters, we're now going to load the data set. We're just loading here the fine-tuning data set. We can print out a sample question. We can simp print out a sample question from the eval data set. You can see here the training data.
It's got a lot of columns. The ones we're using are the question and the answer for fine-tuning. And here's where we set that up into a prompt. So, the prompt is going to have a user message with the user content. And then it's going to have an assistant message with the assistant content. The user content is the question. The assistant is the answer. And here we're just going to format that as a template. So yeah, this is what the fi templating looks like.
We've got user and then we've got uh the assistant. And notice here we'll need this because actually we want to focus the training on the assistant response, not on the user question. So later on, we're going to want to mark this here as the token for indicating the user response. And then we're going to want to mark this as uh the tokens or the string for the start of the assistant response.
Okay, now we're going to start to set up the trainer. So we need to set a training rate. We're going to try a batch size of four. This model probably can fit a larger batch size because it's just 8B. I'm going to use four gradient accumulation steps. My virtual batch site is 32. We'll train for two epochs, one at a constant rate and one decaying uh with a cosine for some annealing. For for a 3B model, we want about 1 e minus 4 of a training rate.
So, put that in here.
And we're going to uh define a current time stamp just for naming the model.
And we're going to set up a run name based on the model name, the fine-tuning data set name, the number of epochs, and a time stamp. We'll calculate the number of training steps, which is total uh total number of rows of data divided by the virtual batch size, which is the batch size times gradient accumulation divided by the epoch. We'll warm up for 1% of the steps, and we'll anneal for the last 50%. We'll print the virtual batch size, the total steps. And we're going to set this custom uh training scheduleuler. Basically, it's going to be constant here when we're below the start of a kneeling and then it's going to uh follow I think either a linear or a cosine drop from there. Yeah, it looks like a linear drop then in learning rate down towards the end which should hopefully smoothly bring us down towards a local minimum. Okay, so virtual batch size 32 14 steps zero warm-up steps because we don't have enough steps for 1% to be meaningful and the analing will start at uh step five. Now you could warm up maybe a little bit more. For example, um where do we have the warm up? Yeah, 01. I mean, we could make it 05 and make it 5% of total steps. And now we've still got one. It looks like I would need to make it larger. I'd need to make it even maybe 0.1. Yeah. So, now we've got one uh training step. I don't like that though cuz I think 10% is too much generally for training or for warm-ups. So I'm just going to leave it and we'll have no warm-up. That's fine. Okay. So now we're going to set up some of the arguments. We'll pass the training batch size. We're going to use that same batch size for the evaluation batch size. Gradient accumulation steps.
Epoch. We're going to log every number of steps. In fact, we're going to log um every 5% of steps, but no less than every single step. Um no less than every 10 steps. Sorry, no more than every 10 steps, I think. Let's see. minimum uh minimum of 10. Yeah, no less than every 10 steps. We're going to evaluate based on steps every 10 steps, but no less than uh no more than every single step. And what else here? Yeah, we're going to use gradient checkpointing.
We'll use re-entrancey. This allows you to speed up. It speeds up the calculations. It's a bit more complicated, so sometimes can give errors sometimes on Quen models, although it worked for me with Quen on Quen 3. Uh so we're going to leave that to tree to true rather um and yeah we'll pass in the max sequence length. And now we'll pass all of these parameters in to the trainer. They're going to be passed in here along with the training and eval set the model and tokenizer and the formatting function that we defined up earlier. Now here's an example of where transformers is different. I should have showed you above but I will in a second.
Normally what you would do to set the optimizer is um you would set theuler here and you would set it equal to um the optimizer and theuler but that is not possible to do with unslaught. So you have to actually retrospectively set the optimizer here and also um make sure that it doesn't get overwritten by later steps of onslaught. So this is necessary because I'm using a customuler. So I have to make sure that is actually being um applied here. I can maybe show you very quickly if I go to my windsurf and I look at the transformers code. When I go to the trainer and the optimizer, you can see it's just being passed in here.
Optimizers equals optimizeruler. That won't work with unsloth. While we're at it, just one difference here as well.
When we're loading the model with transformers, we will load it uh like this with automodel for causal. But this will not work for loading a multimodal model. So it won't work for jamma 3. Uh it won't work uh it might work for jamma 3 4b because that's text only. But for larger it won't work. It won't work as well for um I think mistral you need to use a different and specific way to load it. Whereas unsloth has wrapped things in a way that you can use the same uh loading for every model. So if we look at onslaught here when we load the model pretty much you can pass any model and it's going to load correctly uh just by using let's see this uh fast language model. So it actually supports multimodal models. So that's quite a nice feature. Uh one other difference as well is when we do the PFT there's a slight difference in loading the PFT the parameter efficient fine-tuned model. Here you use fast language model, get PFT model for unsloth and you have these kind of wrappers that allow you at a high level to control vision versus language and control attention versus MLP. Whereas if you're looking at transformers when you get the PFT model, uh it's a little bit more raw which is kind of beneficial because you can target specific modules to turn on. This is the get PFT model.
It's not fast language model. And I think also maybe this might work uh in the case of FI if you're using transformers. Okay, so we have loaded have we loaded? Yeah, we've loaded the model. We've defined all of our training. I need to make sure I run these cells. Uh because I'm going through explaining things to you without checking I've run everything.
So we've now defined the trainer and we have one more thing we need to do which is we need to define how we're going to train on completions only and we need to define this for fi.
So, I'm going to create this and I want to pick FI here and I want to put in the correct start and end of the chat. And to help me here, I can just print out the chat template. And I can see it's actually a bit hard uh to read exactly what I need to include there.
It's probably more helpful if I inspect some of the templated text which says user. So I'm going to copy all of this here and I'm going to paste. And this here is going to be my instruction. I'll give it a second. I think I've lost connection to run pod. I may just need to rerun some of the cells. And then the end portion is this here. Okay. So, I'm just going to save this and I'll download a copy just in case. Uh, so I don't want to lose my work here. And I think everything will be fine. I might just need to rerun a few of the cells. I can actually just restart the kernel, go back to the start of the fine tuning here, and run all of these cells. And the reason why I'm doing this uh chat completion setup is because I just want to train on the completion part. And it's very nicely illustrated when you run these two cells. It's going to show me an example the from the training set example zero. It'll show me the full training row as as it's going to be passed into the trainer. And then this is just going to show me which part we're going to train on. So the loss will only be calculated for those for the assistant tokens. And this is typically the recommended way to uh to train. So yeah, this is the full being passed in. And you can see that all of this part here is masked uh when we're training. So we're only going to train the loss on this last portion. And it looks like everything is uh printing fine here. It doesn't print out the end of text token, which I think is fine. It's important that it does have at least one end of sequence token being generated here. Okay.
So next we're going to start the training. We'll print out the stats. We will make sure we have the data sets and start the training. Uh looks like we have an error. So what's happening here? So I'm not entirely sure what the issue is here. I can check with chat GPT. My inclination is that we may turn off uh the re-entrancey possibly. Well, let's see. And possibly using all three, by the way, would be a better idea. Yeah, it wants us to disable torch compile. I could just try disabling the torch compile, but it's not obvious where exactly I would disable that. Yeah, because I'd have to get into the unsloth code if I want to do that. So, yeah, we might have an issue here.
Um, can I somehow disable compiling like this? Yeah, I'm I'm not entirely sure. It may be hallucinating here, but let's go back and when the model is imported, we need to make sure we do this right at the start of script.
So, when we import the OS, we're going to um disable if trying to tune fi for mini. I'll restart the kernel and try to run this. Um, I'll comment that out so it doesn't spam everyone and we'll see if this works. Maybe it won't and we'll just go to another model. But yeah, this is an example where you might want to use the transformer script uh to get things to work. Okay, so that did work. We managed to disable uh compile and now we're training and the loss is looking good.
You see the the training loss is falling. The validation loss is falling as well. So everything is looking pretty good here. And we're going to save this. Uh let me get a model name that I want to save with. I want a better run name than what we have. So it's going to save it like this.
touch rugby and let's just put that in as a name here. Okay, so we've got 54. We'll name it and yeah, I mean we could push it to hub. Uh why don't we do that? See if this works and run this or is not defined. I don't know why I unccopied that there. Um there we go. And print the run name. And in the meantime, let's um let's check out the logging. Let's check the logs. So to check the logs, it's easiest to connect via SSH. I'll just copy this here, go over to Windsurf, and then in my terminal, I'm going to SSH. I actually need um this is my SSH file, which is in if you go to SSH um directory on your computer, you should find one. You need to put a you need to create an SSH key and put the public key into runpod and then you should be able to connect and once you've connected you can then start up uh tensorboard. So yeah pip install uv install tensorboard move to the workspace and then run in order to see the logs. And if this works and it's up and running, we should then be able to access it via the runpod URL. Yeah. So, it's up and running. I think I could click this, could I? Don't think this is going to bring me to the right page, though, because um this will just bring me to localhost, but that's not going to be accessible. I need instead to go to my run pod pod ID here. And actually the address I need to go to depends on the pod ID which is going to be this. So paste here.
Copy. And now I can check tensorboard. So basically I'm porting in because runpod allows me to port into this. And the run we just did.
Two of these failed. So I'll just show the ones that passed. And the eval loss looks beautiful. Takeoff smoothing. It's falling. And you can see it's it's kind of asmmptoting. So we're kind of getting down to the best point there. The gradient norm is a bit high at the start, but it's good. It's below one.
Then the learning rate is flat then declining. This looks good. And the training loss is flat and declining. So this is all excellent. Uh so everything looks great in terms of these curves here. And now if we go back, we should have by now pushed the model. So the model's been pushed.
That's excellent. And it's now time to inference this model. So I'm going to go all the way back up to the script and I'm going to restart the kernel. And I'll close down this fine tuning section and reopen the val. And I'm going to run these installs. Should be fast. But um actually we should uninstall Unslaught this time because it can cause some conflicts. So let's uninstall that.
Make sure we're logged into hugging face. And this time we're going to use the fine-tuned model. Model slug and fine-tune. Set up the judge. I won't have to re-enter my key cuz it's saved to environment variables. Load the model. Set up evaluation. Run that eval.
We'll inspect that later. Run batching. And yeah, I'm just going to put test equals false here so that we don't accidentally leave testing on. Um, and it looks like we have an error. So something must have failed earlier here.
Yeah.
Um, that actually needs to be trellis.
So my org ID when I put the model in should be this.
The other thing I should maybe have done is saved it locally so I wouldn't have to redownload, but that's okay. So, yeah, it's going to probably have to download the model, which is a bit of a duplication, but that's all right.
Yeah. And when you do install, you need to restart the kernel. So, I forgot about that. So, yeah, I reran the install, then you restart the kernel.
And that's why it's a bit tedious swapping between VLM and Unsworth transformers. But on the other hand, the eval is going to be really fast. And yeah, we're running with VLM here, but we have an issue. So, I'm just going to copy that code. Go back uh to the old trusty here and see what it says. Again, I'm using 40 here. I should probably be using 03, but let's see what happens.
Yeah. So, I'm not going to be able to override it like this. And this is probably the same issue that is happening with Gemma with Gemma 3. Basically, the configuration of the model is not matching what VLM expects. And so what we can do is go back to find the model name which is this one here. Find it on hugging face. And then also if I go to holding face fi mini instruct and yeah that's wrong. It's this one here I want and check the configuration file. So yeah that's the configuration file and let's see the configuration file here in the fi original.
And does this look the same?
Okay. Check I have the right models, which I do. So, everything here looks very similar. Interestingly, the tokenizer is a bit different and the auto model config is a little bit different. So, the auto map is maybe a little bit different and yeah, the architecture is different as well. So, I wonder if I take this here and if I uh paste that in if that's going to help. So, I'll copy this edit. Just check in case there's much different down here. Yeah, there's also this um Okay, that looks similar. This looks similar as well.
So, let's just take this, edit the file, and replace from here. And I'm just going to copy this over. Um, I'm going to save that just in my notes just in case I want to reinject it later. But for now, let's just match what the original is. And this is the original we want to match. And we'll commit those changes. Okay. So, let's see if that does anything. It may or may not may not get to the bottom of this. Restart the kernel and let's try and run it. And as I said, it's not it's not a guarantee at all that this is going to work. If it doesn't work, I'll show you eval with another model. In fact, you've already seen eval. So, this is really just a question of whether we can get it working. It doesn't look good here.
Yeah, it doesn't look good here.
So, this is an example of where you may decide it's worth running with transformers. And what I'll do is I'll just uh I'll just save this here. I'm going to rename it. And I'll put it as uh 54 mini. And I'll download this. And I will put it for those who have access to the repo.
I'll I'll save it uh so that you are able to uh take a look. I'll put it into the fine-tuning folder here and I'll push it up. Um but for now what I will just I'll just quickly show you using the transformer script uh because transformers should work here given it's a texton model. So, if I go to fine-tuning, finetune, and if I upload the transformer script here, we can probably run a pretty fast fine-tune. Uh, starting off with the finetune, we won't we won't even run the eval first. We'll just run the eval afterwards. Uh so let's just run through very quickly um the transformer script and we will make use of variables where we have to like this. So we'll use this as the base model. Uh no we're not going to use the fine-tuned one. We're going to use this one here. So the 54 mini instruct. Everything else is pretty much the same.
We're going to load the model. Um, I don't actually think I want to set the padding to write. I'm just going to print tokenizer padding side. And yeah, we may actually need to manually set the token here because unslaught normally does this.
Um, and that probably pad token is equal to a tokenizer EOS token.
and we'll print tokenizer pad token. Uh so yeah, I'm actually going to clear the GPU and reload it. And yeah, you can see that's interesting. So on slot seems to be setting the padding side to left. We can check it for when we ran it here. This is evaluation, but let's just check in the finetuning. Yeah, unslat is setting it to left whereas the default is actually right. Um, so I'm going to leave it to right. Uh, print the padding token. I have set the pad token here because I think if I print the pad token, um, let's just do this pad token before uh, setting to EOS and yeah, I'll just reload it once more. So pad token before setting to EOS seems to be the EOS token and that implies we don't need to do this. So yeah, I'm not sure what approach on sloth was taken, but it seems like there is actually a pad token and so we don't need to set it. Okay, fine. Uh we'll print the model here. Uh unwrap to base. We actually need to update that because we want to be able to unwrap the fly model. And yep, we've got QKV. And we're going to increase the Laura alpha here a little bit because these are fairly large matrices. And yeah, we're going to target the Q. We're going to target the O. We're not going to target QKV. We're just going to target O. And we are going to try and target um gate up project and down prod.
So we want to add in here gate up proje and we want down project as well. So something like this. Now we should check just to make sure that we are training everything possible. Yeah.
Gator prod. This is probably also a combination um which I don't love. Let's see when we fine-tuned here. Oproj downroge. Yeah, it looks like gunslaught is separating out the down prod down projection which is good. Whereas I'm not going to be Yeah, the down sorry down. Yeah, it's not training gate up.
So, we're actually only training limited numbers of layers. So when I train select here what to train um gate project is not going to be trainable because that's fused well it might be but that's not what I'm going to do and yeah so I'm just going to train these ones for fi and let's see if um yeah so you can see the difference this is actually working here in transformers so it is training the embeddings and that's why we're training 24% of the parameters. Uh so yeah in onslaught this doesn't seem to be actually working for now. Okay, we load the data sets uh the formatting function.
Um and yes, so there is a difference here in Unslaught. You'll remember that after we set the trainer, we then adjusted the mask so that we only train on completion tokens. And with transformers, the way I have it set up, I'm actually doing that beforehand.
And you can see I need to copy over this code here so that we can uh select fi.
And now I have a mistake here. So I need to possibly reload my training data.
Yeah. So this is equivalent. You can see I'm masking everything up until uh the end of the assistant up until the end of the user response. And then I want to keep and train on um the response here.
So everything looks good here. It's just that I'm tokenizing and masking my data set before I pass it into the trainer.
Whereas with Unsloth, you can tokalize afterwards because Unsloth has got a built-in function. So this is uh essentially equivalent. We're just going to train on this portion here.
And now I need to make a few adjustments. Um training batch size of uh eight four two epochs training rate of one E minus 4.
And we will run with the valuation.
Everything here is the same. Technically this notebook supports uh distillation as well although I haven't run it recently. You can check out the distillation video if you want. And we're going to now move to training. It'll be interesting. Yeah, we don't have any issue with torch compile.
I don't know if that's because transformers doesn't use torch compile, but everything looks um looks to be training fine here. Probably our logging we should be logging more frequently uh instead of just logging every five steps. But that's okay. It won't make a difference to the results. And we are going to push this model up to hub and hope for some better results than what we got with Unsloth.
So, I'm going to copy this run name here, paste it in here, comment, and we are going to merge the model, save it, and push it up. And by the way, this time we are saving it locally. So, I should be able to just run the model locally. And while that's working, I will just get ready to run evaluation here by putting in my model name. So the data set we're going to run is this. Or rather, the model we're going to run is this one. And I need to be careful not to run that until it's actually pushed. Okay, I'll just give it a moment to push that model to hub.
Yeah, just while we're waiting for that, we can check out TensorBoard and refresh. And you can see the two runs here.
And yeah, it's interesting. The training and learning rate are the same. The grad norm is a little bit higher for transformers. Um, and the eval loss, we're not logging as frequently, so it's hard to exactly compare, but it does look like the eval loss is a little bit higher for using transformers. And that's not something I would expect because I would think that they are basically uh doing the same thing. And I think we have the same batch size. Um possibly. Oh yeah, we're training embeddings. So actually this makes sense because we're training embeddings. We are able to control more parameters. And yeah, you would expect that would maybe help to give a lower loss, but I guess it isn't helping here.
It's hard to say too much because this is just one run, but that is the main difference between the two runs is that we're training the embeddings in one.
And maybe it's better not to train the embeddings is maybe a tentative conclusion here. So the model is pushed.
Uh we're going to restart the kernel.
Kernel restart kernel. And now we will close down the fine-tuning section.
Scroll to the top of the evals. Make sure we run the installs.
And then we're going to run all of the evaluations. I'll restart the kernel after doing the installs. And now proceed to evaluate. So we're hoping to have more luck this time. Prepare the data set. And let's see if the model will load. If it does load, we should get out some answers. Okay. So yeah, this time everything is loading correctly. So we don't have the same issue as we did with Unsloth. And you can see unfortunately I wasn't able to fix the configuration file for making the onslaught trained model work with VLM. But this eval is going to work. So that's good news. Uh we'll continue running these cells. Set up the batch evaluation and then we will evaluate on this comprehensive touch rugby set. And let's see what happens here.
Just make sure that test is equal to false. And we're trying to beat our baseline score which was about um five or six if I remember correctly. Let me just open up and see what it was we got.
Um so that's the Mistral small results and we want the FI results which is this one. So we got to beat 5.33. As I said I'm not sure we will just because this I haven't spent a lot of time on data prep and augmenting. So we may or may not do better. And seven.
Okay. So we're doing better. So we're up from that's just one, but let's see if we run it again.
Eight 7 seven. Okay. So definitely the finetuning has improved uh the results here. We've gone from seven.3 with a variance of 44 uh up from 5.3. So we are seeing a positive effect here. Fine-tuning. We're of course a long way off getting all of the answers correct. So there's quite a bit of work to do um in terms of improving this model's performance up to where we would want it. But you can see uh how we ran the eval. We ran the finetuning and we ran the eval again and managed to get uh everything working. So I'm going to save the latest copy of this uh file here and I'll download it and it'll be uploaded to the finetuning folder uh for those of you who want to run uh with the FI model. All right, folks. That's it. Um, I ended up doing quite a detailed and realistic run through of the problems you go into when you're trying to fine-tune. Hopefully, you've more appreciation for transformers versus unsloth and also the benefits of doing the VLM approach for evaluation. It really is a lot faster than having to wait for results if you're going to run inference with Unslaught or transformers. The scripts are in the advanced fine-tuning repo. You can purchase access by going to trellis.comadvanced-finetuning. And I do plan on building on this further to help fine-tune for reasoning uh particularly in verbal which are non-quantitative type applications where it's more difficult to do the reasoning uh type fine-tuning. As usual, let me know if you have any questions below in the comments. Cheers, folks.
